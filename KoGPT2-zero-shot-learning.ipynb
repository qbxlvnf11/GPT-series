{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.63.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.10.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 01:30:47.640881: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-07 01:30:47.736866: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-11-07 01:30:47.736884: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-07 01:30:47.754265: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-07 01:30:48.228942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-11-07 01:30:48.228995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-11-07 01:30:48.229002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Load Review Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Wittgensteinian--KR3-91924806189fc1d6\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Wittgensteinian___parquet/Wittgensteinian--KR3-91924806189fc1d6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "kr3 = load_dataset(\"Wittgensteinian/KR3\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:28:20.745463Z",
     "iopub.status.busy": "2022-03-27T07:28:20.745095Z",
     "iopub.status.idle": "2022-03-27T07:28:29.503287Z",
     "shell.execute_reply": "2022-03-27T07:28:29.501786Z",
     "shell.execute_reply.started": "2022-03-27T07:28:20.745367Z"
    }
   },
   "outputs": [],
   "source": [
    "kr3 = kr3.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:28:31.281029Z",
     "iopub.status.busy": "2022-03-27T07:28:31.280333Z",
     "iopub.status.idle": "2022-03-27T07:28:31.288544Z",
     "shell.execute_reply": "2022-03-27T07:28:31.287778Z",
     "shell.execute_reply.started": "2022-03-27T07:28:31.280991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'Review'],\n",
       "    num_rows: 641762\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:28:35.635911Z",
     "iopub.status.busy": "2022-03-27T07:28:35.635321Z",
     "iopub.status.idle": "2022-03-27T07:28:35.645611Z",
     "shell.execute_reply": "2022-03-27T07:28:35.644915Z",
     "shell.execute_reply.started": "2022-03-27T07:28:35.635871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rating': Value(dtype='int32', id=None),\n",
       " 'Review': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove ambiguous reviews whose rating is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:28:36.402333Z",
     "iopub.status.busy": "2022-03-27T07:28:36.401811Z",
     "iopub.status.idle": "2022-03-27T07:28:42.264350Z",
     "shell.execute_reply": "2022-03-27T07:28:42.263606Z",
     "shell.execute_reply.started": "2022-03-27T07:28:36.402296Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Wittgensteinian___parquet/Wittgensteinian--KR3-91924806189fc1d6/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c212988a2a873f5e.arrow\n"
     ]
    }
   ],
   "source": [
    "kr3_binary = kr3.filter(lambda x: x['Rating'] != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'Review'],\n",
       "    num_rows: 459021\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - KoGPT2 Tokenzier & Model\n",
    "    - GPT-2 trained on Korean corpus: https://github.com/SKT-AI/KoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\", pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vocab of tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51201"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Token Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "review = kr3_binary['Review'][idx]\n",
    "label = kr3_binary['Rating'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'숙성 돼지고기 전문점입니다. 건물 모양 때문에 매장 모양도 좀 특이하지만 쾌적한 편이고 살짝 레트로 감성으로 분위기 잡아놨습니다. 모든 직원분들께서 전부 가능하다고 멘트 쳐주시며, 고기는 초반 커팅까지는 구워주십니다. 가격 저렴한 편 아니지만 맛은 준수합니다. 등심덧살이 인상 깊었는데 구이로 별로일 줄 알았는데 육향 짙고 얇게 저며 뻑뻑하지 않았습니다. 하이라이트는 된장찌개. 진짜 굿입니다. 버터 간장밥, 골뱅이 국수 등 나중에 더 맛봐야 할 것들은 남겨뒀습니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:29:01.761230Z",
     "iopub.status.busy": "2022-03-27T07:29:01.760974Z",
     "iopub.status.idle": "2022-03-27T07:29:09.135311Z",
     "shell.execute_reply": "2022-03-27T07:29:09.134499Z",
     "shell.execute_reply.started": "2022-03-27T07:29:01.761195Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_review = tokenizer(review, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[44381, 26367,  6958, 10161,  8191, 21154, 10637,  9777,  9355, 13669,\n",
       "          9777,  7235, 11732, 15846, 11686, 43752,  9266,  9466, 20387, 10286,\n",
       "         11714,  9244, 12041, 33684, 13364,  7130, 16691,  9548, 18401,  7671,\n",
       "          7285, 23916, 17483,  9826, 12524,   739, 18221, 13673,  8236,  7888,\n",
       "          9061,  9065,  9446, 18622, 10114,  8614, 12109, 26089,  8236,  7895,\n",
       "         12521, 11562, 29932,  9266, 22804, 32837, 22033, 37194,  9030,  7894,\n",
       "          7216, 16912, 15464,  9958, 16693,  9073, 11434, 15126,  8149,  9566,\n",
       "          9181, 31231,  9719,  8721, 14591,  6889, 25446,  9265,  7530,   739,\n",
       "          7723,  7723,  9328, 10171, 16691,  9078,  9131, 51000,  9498,  8168,\n",
       "          8326,  6841,   389, 23971, 15669, 21154,  9848,  8539, 49375,  7605,\n",
       "           387, 10187,  7616,  8146,  9092,  7847,  9030, 13348,  9267, 11355,\n",
       "          7661,  7991,  9337, 24860, 18525,  7268, 16691]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feed the model with the tokenized review, and the model gives an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(**tokenized_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output tensor represents: the unnormalized (before passing softmax) probabilities of each token coming out in the end of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = y.logits[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:29:09.138001Z",
     "iopub.status.busy": "2022-03-27T07:29:09.137501Z",
     "iopub.status.idle": "2022-03-27T07:29:09.631066Z",
     "shell.execute_reply": "2022-03-27T07:29:09.630343Z",
     "shell.execute_reply.started": "2022-03-27T07:29:09.137954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.5961, -6.9506, -5.8478,  ..., -1.8356, -5.1506, -3.0455],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9394)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:29:09.632752Z",
     "iopub.status.busy": "2022-03-27T07:29:09.632489Z",
     "iopub.status.idle": "2022-03-27T07:29:09.655243Z",
     "shell.execute_reply": "2022-03-27T07:29:09.654487Z",
     "shell.execute_reply.started": "2022-03-27T07:29:09.632718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그리고'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Compare model's score of *positive* and *negative* tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select a pair of tokens to represent *positive* and *negative* respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:29:14.097116Z",
     "iopub.status.busy": "2022-03-27T07:29:14.096879Z",
     "iopub.status.idle": "2022-03-27T07:29:14.105252Z",
     "shell.execute_reply": "2022-03-27T07:29:14.104460Z",
     "shell.execute_reply.started": "2022-03-27T07:29:14.097080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁최고', '입', '니', '다']\n",
      "['▁별로', '입', '니', '다']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('최고입니다')) \n",
    "print(tokenizer.tokenize('별로입니다')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:29:14.106936Z",
     "iopub.status.busy": "2022-03-27T07:29:14.106636Z",
     "iopub.status.idle": "2022-03-27T07:29:14.114369Z",
     "shell.execute_reply": "2022-03-27T07:29:14.113480Z",
     "shell.execute_reply.started": "2022-03-27T07:29:14.106901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10281]\n",
      "[15126]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('최고'))\n",
    "print(tokenizer.encode('별로'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.1346, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[10281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9310, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[15126]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- label==1 is for positive reviews, and 0 is for negative reviews\n",
    "- The probability assigned to token 'Best' higher than those assigned to token 'Not good'\n",
    "- We can predict the input review as *positive* or *negative* by comparing model's score of 'Best' and 'Not good' tokens as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:29:14.115945Z",
     "iopub.status.busy": "2022-03-27T07:29:14.115596Z",
     "iopub.status.idle": "2022-03-27T07:29:14.124460Z",
     "shell.execute_reply": "2022-03-27T07:29:14.123641Z",
     "shell.execute_reply.started": "2022-03-27T07:29:14.115912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((prediction[10281] > prediction[15126]) == label).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Tokenization\n",
    "- *truncation* is when you truncate(=cut) the input text because it's too long (i.e. it exceeds the max_length).\n",
    "- *padding* is when you add extra tokens in the end of the input text to create a batch. We do not pad here. Instead, we set dynamic padding when we create PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:29:14.127724Z",
     "iopub.status.busy": "2022-03-27T07:29:14.127306Z",
     "iopub.status.idle": "2022-03-27T07:30:30.409672Z",
     "shell.execute_reply": "2022-03-27T07:30:30.408867Z",
     "shell.execute_reply.started": "2022-03-27T07:29:14.127689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016025066375732422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 23,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 460,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd5966fddbd4433a8082493cd0f15d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/460 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize\n",
    "def tokenize_func(x):\n",
    "    return tokenizer(x['Review'], max_length=256, truncation=True)\n",
    "\n",
    "kr3_tokenized = kr3_binary.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new features: `attention_mask` and `input_ids`. These are the parameters for the model(GPT-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:30:30.412911Z",
     "iopub.status.busy": "2022-03-27T07:30:30.412706Z",
     "iopub.status.idle": "2022-03-27T07:30:30.422037Z",
     "shell.execute_reply": "2022-03-27T07:30:30.421334Z",
     "shell.execute_reply.started": "2022-03-27T07:30:30.412885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'Review', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 459021\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we do not need the feature `Review`, we remove it. Plus, we set the format of this dataset as 'torch', as we're going to use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:30:30.423700Z",
     "iopub.status.busy": "2022-03-27T07:30:30.423419Z",
     "iopub.status.idle": "2022-03-27T07:30:30.605338Z",
     "shell.execute_reply": "2022-03-27T07:30:30.604445Z",
     "shell.execute_reply.started": "2022-03-27T07:30:30.423652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Rating', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 459021\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr3_tokenized = kr3_tokenized.remove_columns(['Review'])\n",
    "kr3_tokenized.set_format('torch')\n",
    "kr3_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - KoGPT2 Inference using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We make PyTorch dataloader. See the exmaple batch.  \n",
    "    - `Rating[i]` represents the label (0 or 1) for (*i+1*)th review in the batch.\n",
    "    - `attention_mask[i]` and `input_ids[i]` are the tokenized (*i+1*)th review in the batch.\n",
    "\n",
    "- Dynamic Padding\n",
    "> We set dynamic padding using `DataCollatorWithPadding` from `transformers`. Dynamic padding pads to the longest sequence in the batch, instead of padding to certain fixed length. In the example below, we can deduce that the longest sequence in the batch had a length of 117."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_loader = DataLoader(kr3_tokenized, batch_size=batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:30:48.944635Z",
     "iopub.status.busy": "2022-03-27T07:30:48.944307Z",
     "iopub.status.idle": "2022-03-27T07:30:49.594839Z",
     "shell.execute_reply": "2022-03-27T07:30:49.594122Z",
     "shell.execute_reply.started": "2022-03-27T07:30:48.944598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rating': torch.Size([8]), 'input_ids': torch.Size([8, 117]), 'attention_mask': torch.Size([8, 117])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "print({k:v.size() for k,v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:31:00.403785Z",
     "iopub.status.busy": "2022-03-27T07:31:00.403252Z",
     "iopub.status.idle": "2022-03-27T07:31:00.412824Z",
     "shell.execute_reply": "2022-03-27T07:31:00.411449Z",
     "shell.execute_reply.started": "2022-03-27T07:31:00.403738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU available')\n",
    "else:\n",
    "    print('GPU not ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:31:03.107407Z",
     "iopub.status.busy": "2022-03-27T07:31:03.106930Z",
     "iopub.status.idle": "2022-03-27T07:31:07.663130Z",
     "shell.execute_reply": "2022-03-27T07:31:07.662441Z",
     "shell.execute_reply.started": "2022-03-27T07:31:03.107361Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We predict the sentiment of each review via inference of GPT-2. This loop will take some time.\n",
    "\n",
    "> `input_lens` represents the length of each input text. This is used to obtain the token predicted right after the input text. \n",
    "\n",
    "> If you run out of GPU memory, try to reduce `max_length` (in tokenization) or `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T07:31:09.207344Z",
     "iopub.status.busy": "2022-03-27T07:31:09.206765Z",
     "iopub.status.idle": "2022-03-27T08:28:38.372469Z",
     "shell.execute_reply": "2022-03-27T08:28:38.371595Z",
     "shell.execute_reply.started": "2022-03-27T07:31:09.207297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01332855224609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 23,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 57378,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db66919653854dd6ac5e685fd0b811c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = [[0,0],[0,0]]\n",
    "\n",
    "for batch in tqdm(data_loader):\n",
    "    batch = {k:v.to(device) for k,v in batch.items()} # move the data to the GPU\n",
    "    y = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask']) # forward\n",
    "    input_lens = batch['attention_mask'].sum(axis=1) # length of inputs in the batch\n",
    "\n",
    "    for i in range(len(batch['Rating'])):\n",
    "        next_token_prediction = y.logits[i, input_lens[i]-1] # output of the model for single review\n",
    "\n",
    "        # prediction result\n",
    "        predicted_label = (next_token_prediction[10281] > next_token_prediction[15126]).item() \n",
    "        true_label = batch['Rating'][i].item()\n",
    "        confusion_matrix[true_label][predicted_label] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T08:31:39.491203Z",
     "iopub.status.busy": "2022-03-27T08:31:39.490403Z",
     "iopub.status.idle": "2022-03-27T08:31:39.497477Z",
     "shell.execute_reply": "2022-03-27T08:31:39.496758Z",
     "shell.execute_reply.started": "2022-03-27T08:31:39.491165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33148,  37762],\n",
       "       [ 76503, 311608]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "confusion_np_matrix = np.array(confusion_matrix)\n",
    "confusion_np_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-27T08:31:40.928994Z",
     "iopub.status.busy": "2022-03-27T08:31:40.928483Z",
     "iopub.status.idle": "2022-03-27T08:31:40.941875Z",
     "shell.execute_reply": "2022-03-27T08:31:40.940882Z",
     "shell.execute_reply.started": "2022-03-27T08:31:40.928949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7510680339243738\n",
      "Precision for positive: 0.8919140166585569\n",
      "Precision for negative: 0.30230458454551257\n",
      "Recall for positive: 0.8028837111032668\n",
      "Recall for negative: 0.46746580172049074\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', confusion_np_matrix.diagonal().sum() / confusion_np_matrix.sum())\n",
    "print('Precision for positive:', confusion_np_matrix[1,1] / confusion_np_matrix[:,1].sum())\n",
    "print('Precision for negative:', confusion_np_matrix[0,0] / confusion_np_matrix[:,0].sum())\n",
    "print('Recall for positive:', confusion_np_matrix[1,1] / confusion_np_matrix[1,:].sum())\n",
    "print('Recall for negative:', confusion_np_matrix[0,0] / confusion_np_matrix[0,:].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
